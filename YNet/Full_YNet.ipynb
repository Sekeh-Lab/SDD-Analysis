{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full_YNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drHa3e_dYqta",
        "outputId": "1a5c77ba-5167-4bd8-cf1f-6b5032973d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/YNet\n",
            "/content/drive/MyDrive/YNet\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "%cd \"/content\"\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "# %cd \"./drive/MyDrive/YNet/utils\"\n",
        "%cd \"./drive/MyDrive/YNet/\"\n",
        "\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.48 --force-reinstall\n",
        "!pip install pyyaml==5.3.1 --force-reinstall\n",
        "!pip install torch==1.5.1 --force-reinstall\n",
        "!pip install --no-dependencies opencv-python==4.4.0.42 --force-reinstall\n",
        "!pip install --no-dependencies scipy==1.5.0 --force-reinstall\n",
        "\n",
        "!pip install --no-dependencies torchvision==0.6.1 --force-reinstall\n",
        "!pip install --no-dependencies torchtext==0.6 --force-reinstall\n",
        "# !pip install  --no-dependencies --ignore-installed PyYAML\n",
        "!pip install  --no-dependencies segmentation_models_pytorch==0.1.0\n",
        "!pip install pretrainedmodels\n",
        "!pip install efficientnet-pytorch==0.5.1\n",
        "!pip install pandas==1.0.5 --force-reinstall\n",
        "!pip3 install pickle5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jq7WmuEfYz38",
        "outputId": "09eab868-aeb7-40ff-a0f3-9739af311716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tqdm==4.48\n",
            "  Downloading tqdm-4.48.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▉                           | 10 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 20 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 30 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 40 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 51 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 61 kB 3.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 67 kB 2.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.63.0\n",
            "    Uninstalling tqdm-4.63.0:\n",
            "      Successfully uninstalled tqdm-4.63.0\n",
            "Successfully installed tqdm-4.48.0\n",
            "Collecting pyyaml==5.3.1\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 2.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44636 sha256=5277aefb499a9aeeb5b19518d5c549418969c5d76062b0f52339166a74f60532\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.3.1\n",
            "Collecting torch==1.5.1\n",
            "  Downloading torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.2 MB 12 kB/s \n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 34.2 MB/s \n",
            "\u001b[?25hCollecting future\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 59.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=64cba871a6a86a29870f0272ce263ee04493dd56ecb70e556abc7c678d2a03b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: numpy, future, torch\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.5.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.5.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.5.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed future-0.18.2 numpy-1.21.5 torch-1.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python==4.4.0.42\n",
            "  Downloading opencv_python-4.4.0.42-cp37-cp37m-manylinux2014_x86_64.whl (49.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.4 MB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "Successfully installed opencv-python-4.4.0.42\n",
            "Collecting scipy==1.5.0\n",
            "  Downloading scipy-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.5.0\n",
            "Collecting torchvision==0.6.1\n",
            "  Downloading torchvision-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 2.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "Successfully installed torchvision-0.6.1\n",
            "Collecting torchtext==0.6\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "Successfully installed torchtext-0.6.0\n",
            "Collecting segmentation_models_pytorch==0.1.0\n",
            "  Downloading segmentation_models_pytorch-0.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 604 kB/s \n",
            "\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n",
            "Successfully installed segmentation-models-pytorch-0.1.0\n",
            "Collecting pretrainedmodels\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (1.5.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (0.6.1)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (4.48.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (0.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (7.1.2)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=91328f9ab304cdd31245435e3dd9342241d474fd22b95758c2a3f1c24df0e6ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "segmentation-models-pytorch 0.1.0 requires efficientnet-pytorch>=0.5.1, which is not installed.\u001b[0m\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n",
            "Collecting efficientnet-pytorch==0.5.1\n",
            "  Downloading efficientnet_pytorch-0.5.1.tar.gz (12 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.5.1) (1.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.5.1) (0.18.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.5.1) (1.21.5)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.5.1-py3-none-any.whl size=11770 sha256=67b119b87b18eb9bc9f03abdf592ff87a5b9ad441ef17e85c11cc5dfd57ae398\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/ca/4d/54f89984720571db345c6a040fbd26268bd23a92ddbd84f201\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.5.1\n",
            "Collecting pandas==1.0.5\n",
            "  Downloading pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 3.9 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting pytz>=2017.2\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 50.6 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.6.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 62.8 MB/s \n",
            "\u001b[?25hCollecting six>=1.5\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: six, pytz, python-dateutil, numpy, pandas\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.6.0 requires sentencepiece, which is not installed.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.5 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.5 pandas-1.0.5 python-dateutil-2.8.2 pytz-2021.3 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 81 kB 3.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 92 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 102 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 112 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 122 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 133 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 143 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 153 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 163 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 174 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 184 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 194 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 204 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 215 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 225 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 235 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 245 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 2.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "%cd \"/content\"\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "# %cd \"./drive/MyDrive/YNet/utils\"\n",
        "%cd \"./drive/MyDrive/YNet/\"\n",
        "\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9v0k8c2blVz",
        "outputId": "480a8107-b747-4f32-a0fa-f9943041f321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/YNet\n",
            "/content/drive/MyDrive/YNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.testing import assert_frame_equal\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import argparse\n",
        "import torch\n",
        "from model import YNet\n",
        "import pickle5 as pickle\n"
      ],
      "metadata": {
        "id": "Z0t3hspGY1W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "CttROrIEY35D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502d242b-e28d-4292-fe32-c3fc38d0f0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################## TRAINING #####################################"
      ],
      "metadata": {
        "id": "QyZ168KRY5vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENT_NAME = 'all/sdd_trajnet/0'  # arbitrary name for this experiment\n",
        "\n",
        "\n",
        "# CONFIG_FILE_PATH = 'config/sdd_trajnet.yaml'  # yaml config file containing all the hyperparameters\n",
        "# DATASET_NAME = 'sdd'\n",
        "CONFIG_FILE_PATH = 'config/ind.yaml'  # yaml config file containing all the hyperparameters\n",
        "DATASET_NAME = 'ind'\n",
        "\n",
        "# TRAIN_IMAGE_PATH = 'data/inD/train'\n",
        "VAL_IMAGE_PATH = 'data/inD/test'\n",
        "TEST_IMAGE_PATH = 'data/inD/test'  # only needed for YNet, PECNet ignores this value\n",
        "\n",
        "TRAIN_IMAGE_PATH = 'data/SDD/train'\n",
        "# VAL_IMAGE_PATH = 'data/SDD/test'\n",
        "# TEST_IMAGE_PATH = 'data/SDD/test'  # only needed for YNet, PECNet ignores this value\n",
        "\n",
        "\n",
        "OBS_LEN = 8  # in timesteps\n",
        "PRED_LEN = 12  # in timesteps\n",
        "NUM_GOALS = 20  # K_e\n",
        "NUM_TRAJ = 1  # K_a\n",
        "\n",
        "ROUNDS = 1  # Y-net is stochastic. How often to evaluate the whole dataset\n",
        "BATCH_SIZE = 8"
      ],
      "metadata": {
        "id": "lJDGQ16jY5kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENT_NAME = 'peds/sdd/raw/1'  # arbitrary name for this experiment\n",
        "# EXPERIMENT_NAME = 'peds/sdd/corrected/0'  # arbitrary name for this experiment\n",
        "# EXPERIMENT_NAME = 'peds/ind/0'  # arbitrary name for this experiment\n",
        "\n",
        "EXPERIMENT_NAME = 'cars/sdd/raw/2'  # arbitrary name for this experiment\n",
        "# EXPERIMENT_NAME = 'cars/sdd/corrected/0'  # arbitrary name for this experiment\n",
        "# EXPERIMENT_NAME = 'cars/ind/0'  # arbitrary name for this experiment\n",
        "\n",
        "# EXPERIMENT_NAME = 'bicycles/sdd/raw/11'  # arbitrary name for this experiment\n",
        "# EXPERIMENT_NAME = 'bicycles/sdd/corrected/0'  # arbitrary name for this experiment\n",
        "# EXPERIMENT_NAME = 'bicycles/ind/10'  # arbitrary name for this experiment\n",
        "\n",
        "# EXPERIMENT_NAME = 'all/sdd/raw/0'  # arbitrary name for this experiment\n",
        "# EXPERIMENT_NAME = 'all/sdd/corrected/10'  # arbitrary name for this experiment\n",
        "# EXPERIMENT_NAME = 'all/ind/0'  # arbitrary name for this experiment\n",
        "\n",
        "# df_train = pd.read_csv(\"data/SDD_new/raw/pedestrians/trainSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/SDD_new/raw/pedestrians/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/SDD_new/raw/pedestrians/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "df_train = pd.read_csv(\"data/SDD_new/raw/cars/trainSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/SDD_new/raw/cars/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/SDD_new/raw/cars/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "# df_train = pd.read_csv(\"data/SDD_new/raw/bicycles/trainSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/SDD_new/raw/bicycles/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/SDD_new/raw/bicycles/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "# df_train = pd.read_csv(\"data/SDD_new/raw/all/trainSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/SDD_new/raw/all/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/SDD_new/raw/all/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "\n",
        "# df_train = pd.read_csv(\"data/SDD_new/corrected/pedestrians/trainSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/SDD_new/corrected/pedestrians/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/SDD_new/corrected/pedestrians/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "# df_train = pd.read_csv(\"data/SDD_new/corrected/cars/trainSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/SDD_new/corrected/cars/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/SDD_new/corrected/cars/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "# df_train = pd.read_csv(\"data/SDD_new/corrected/all/trainSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/SDD_new/corrected/all/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/SDD_new/corrected/all/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "\n",
        "# df_train = pd.read_csv(\"data/inD/pedestrians/trainIND.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/inD/pedestrians/testIND.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/inD/pedestrians/testIND.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "# df_train = pd.read_csv(\"data/inD/cars/trainIND.csv\", header = 0, delimiter = r' ')\n",
        "df_val = pd.read_csv(\"data/inD/cars/testIND.csv\", header = 0, delimiter = r' ')\n",
        "df_test = pd.read_csv(\"data/inD/cars/testIND.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "# df_train = pd.read_csv(\"data/inD/bicycles/trainIND.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/inD/bicycles/testIND.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/inD/bicycles/testIND.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "# df_train = pd.read_csv(\"data/inD/all/trainIND.csv\", header = 0, delimiter = r' ')\n",
        "# df_val = pd.read_csv(\"data/inD/all/testIND.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = pd.read_csv(\"data/inD/all/testIND.csv\", header = 0, delimiter = r' ')\n",
        "\n",
        "\n",
        "df_train = df_train[['trackId','frame','x','y','sceneId','metaId']]\n",
        "df_val = df_val[['trackId','frame','x','y','sceneId','metaId']]"
      ],
      "metadata": {
        "id": "gbwbS8U9ZmQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(CONFIG_FILE_PATH) as file:\n",
        "    params = yaml.load(file, Loader=yaml.FullLoader)\n",
        "# experiment_name = CONFIG_FILE_PATH.split('.yaml')[0].split('config/')[1]\n",
        "experiment_name = EXPERIMENT_NAME\n",
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRD7lX1kZkBc",
        "outputId": "2b6f9464-0c2d-4f3e-c76f-1077ff9bd5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CWS_params': {'ratio': 2, 'rot': True, 'sigma_factor': 6},\n",
              " 'batch_size': 8,\n",
              " 'decoder_channels': [64, 64, 64, 32, 32],\n",
              " 'encoder_channels': [32, 32, 64, 64, 64],\n",
              " 'kernlen': 31,\n",
              " 'learning_rate': 0.0001,\n",
              " 'loss_scale': 1000,\n",
              " 'nsig': 4,\n",
              " 'num_epochs': 60,\n",
              " 'rel_threshold': 0.01,\n",
              " 'resize': 0.33,\n",
              " 'segmentation_model_fp': 'segmentation_models/inD_segmentation.pth',\n",
              " 'semantic_classes': 6,\n",
              " 'temperature': 1.0,\n",
              " 'unfreeze': 150,\n",
              " 'use_CWS': True,\n",
              " 'use_TTST': True,\n",
              " 'use_features_only': False,\n",
              " 'viz_epoch': 10,\n",
              " 'waypoints': [11]}"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_train0.head())\n",
        "print(df_train.head())\n",
        "# print(df_train0.columns)\n",
        "print(df_train.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boU3TKp2Z0dj",
        "outputId": "8e417176-c350-45e9-87e1-aeb6b12c14f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   trackId  frame       x      y      sceneId  metaId\n",
            "0       86   2508  1389.0  649.0  bookstore_0       0\n",
            "1       86   2520  1389.0  649.0  bookstore_0       0\n",
            "2       86   2532  1389.0  649.0  bookstore_0       0\n",
            "3       86   2544  1389.0  649.0  bookstore_0       0\n",
            "4       86   2556  1389.0  649.0  bookstore_0       0\n",
            "Index(['trackId', 'frame', 'x', 'y', 'sceneId', 'metaId'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.device('cuda:0')"
      ],
      "metadata": {
        "id": "o-JvITTlZ3ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YNet(obs_len=OBS_LEN, pred_len=PRED_LEN, params=params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAOGeTH_Z5MU",
        "outputId": "1840bae2-92b2-44fc-e328-a30be6905a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'segmentation_models_pytorch.encoders.resnet.ResNetEncoder' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'segmentation_models_pytorch.base.modules.Conv2dReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'segmentation_models_pytorch.base.modules.Activation' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(df_train, df_val, params, train_image_path=TRAIN_IMAGE_PATH, val_image_path=VAL_IMAGE_PATH,\n",
        "            experiment_name=EXPERIMENT_NAME, batch_size=BATCH_SIZE, num_goals=NUM_GOALS, num_traj=NUM_TRAJ, \n",
        "            device=cuda, dataset_name=DATASET_NAME)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "tPoI4Ug8Z5Ha",
        "outputId": "a6843dcd-e035-47a9-c6bb-f636cde71a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is:  cuda:0\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.4 GB\n",
            "Cached:    8.7 GB\n",
            "Preprocess data\n",
            "data/SDD/train/bookstore_0/reference.png\n",
            "data/SDD/train/bookstore_1/reference.png\n",
            "data/SDD/train/bookstore_2/reference.png\n",
            "data/SDD/train/bookstore_3/reference.png\n",
            "data/SDD/train/deathCircle_0/reference.png\n",
            "data/SDD/train/deathCircle_1/reference.png\n",
            "data/SDD/train/deathCircle_3/reference.png\n",
            "data/SDD/train/gates_0/reference.png\n",
            "data/SDD/train/gates_1/reference.png\n",
            "data/SDD/train/gates_4/reference.png\n",
            "data/SDD/train/gates_8/reference.png\n",
            "data/SDD/train/nexus_1/reference.png\n",
            "data/SDD/train/nexus_4/reference.png\n",
            "data/SDD/train/nexus_7/reference.png\n",
            "data/SDD/train/nexus_8/reference.png\n",
            "data/SDD/train/nexus_9/reference.png\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-1b23da1c8ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.train(df_train, df_val, params, train_image_path=TRAIN_IMAGE_PATH, val_image_path=VAL_IMAGE_PATH,\n\u001b[1;32m      2\u001b[0m             \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEXPERIMENT_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_goals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_GOALS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_traj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TRAJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             device=cuda, dataset_name=DATASET_NAME)\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/YNet/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, val_data, params, train_image_path, val_image_path, experiment_name, batch_size, num_goals, num_traj, device, dataset_name)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# Load train images and augment train data and images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         df_train, train_images = augment_data(train_data, image_path=train_image_path, image_file=image_file_name,\n\u001b[0;32m--> 271\u001b[0;31m                                               seg_mask=seg_mask)\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Load val scene images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/YNet/utils/preprocessing.py\u001b[0m in \u001b[0;36maugment_data\u001b[0;34m(data, image_path, images, image_file, seg_mask)\u001b[0m\n\u001b[1;32m    281\u001b[0m                                 \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                         \u001b[0mdata_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msceneId\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m                         \u001b[0;31m# image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mrot_angle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk2rot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/YNet/utils/preprocessing.py\u001b[0m in \u001b[0;36mrot\u001b[0;34m(df, image, k)\u001b[0m\n\u001b[1;32m    199\u001b[0m \t'''\n\u001b[1;32m    200\u001b[0m         \u001b[0mxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ndim'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GrLPFwS88Gm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_FILE_PATH = 'config/sdd_trajnet.yaml'  # yaml config file containing all the hyperparameters\n",
        "DATASET_NAME = 'sdd'\n",
        "\n",
        "# TEST_DATA_PATH = 'data/SDD/test_trajnet.pkl'\n",
        "TEST_IMAGE_PATH = 'data/SDD/test'  # only needed for YNet, PECNet ignores this value\n",
        "OBS_LEN = 8  # in timesteps\n",
        "PRED_LEN = 12  # in timesteps\n",
        "NUM_GOALS = 20  # K_e\n",
        "NUM_TRAJ = 1  # K_a\n",
        "\n",
        "ROUNDS = 1  # Y-net is stochastic. How often to evaluate the whole dataset\n",
        "BATCH_SIZE = 8"
      ],
      "metadata": {
        "id": "5lef5dpBjXUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(CONFIG_FILE_PATH) as file:\n",
        "    params = yaml.load(file, Loader=yaml.FullLoader)\n",
        "# experiment_name = CONFIG_FILE_PATH.split('.yaml')[0].split('config/')[1]\n",
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvMKs9qZjbrx",
        "outputId": "fd7c63c5-e0fd-40de-cc73-6bb976e16a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CWS_params': {'ratio': 2, 'rot': True, 'sigma_factor': 6},\n",
              " 'batch_size': 8,\n",
              " 'decoder_channels': [64, 64, 64, 32, 32],\n",
              " 'encoder_channels': [32, 32, 64, 64, 64],\n",
              " 'kernlen': 31,\n",
              " 'learning_rate': 0.0001,\n",
              " 'loss_scale': 1000,\n",
              " 'nsig': 4,\n",
              " 'num_epochs': 60,\n",
              " 'rel_threshold': 0.01,\n",
              " 'resize': 0.33,\n",
              " 'segmentation_model_fp': 'segmentation_models/inD_segmentation.pth',\n",
              " 'semantic_classes': 6,\n",
              " 'temperature': 1.0,\n",
              " 'unfreeze': 150,\n",
              " 'use_CWS': True,\n",
              " 'use_TTST': True,\n",
              " 'use_features_only': False,\n",
              " 'viz_epoch': 10,\n",
              " 'waypoints': [11]}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(TEST_DATA_PATH, \"rb\") as fh:\n",
        "#   df_test2 = pickle.load(fh)\n",
        "# df_test = pd.read_csv(\"data/SDD_new/raw/cars/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "# # df_val = pd.read_csv(\"data/SDD_new/raw/cars/testSDD.csv\", header = 0, delimiter = r' ')\n",
        "# df_test = df_test[['trackId','frame','x','y','sceneId','metaId']]\n",
        "# # df_val = df_val[['trackId','frame','x','y','sceneId','metaId']]\n",
        "\n",
        "# print(df_test2.head())\n",
        "print(df_test.head())\n",
        "\n",
        "# print(df_test2.shape)\n",
        "print(df_test.shape)\n",
        "# print(df_test2.sceneId.unique())\n",
        "print(df_test.sceneId.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDNTsWP0je0x",
        "outputId": "0355e42c-b3e1-4c75-8527-4681ce66641a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   frame  trackId           x           y sceneId  metaId\n",
            "0   1450     32.0  439.416347  737.290175  video6       0\n",
            "1   1460     32.0  453.959532  729.257129  video6       0\n",
            "2   1470     32.0  468.780867  722.204534  video6       0\n",
            "3   1480     32.0  483.918477  716.098598  video6       0\n",
            "4   1490     32.0  499.272886  710.735184  video6       0\n",
            "(8340, 6)\n",
            "['video6' 'video16' 'video17' 'video28' 'video29' 'video32']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = YNet(obs_len=OBS_LEN, pred_len=PRED_LEN, params=params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96cZ8jcvjksp",
        "outputId": "9c79f942-bc58-46ba-8045-624ab790728a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'segmentation_models_pytorch.encoders.resnet.ResNetEncoder' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'segmentation_models_pytorch.base.modules.Conv2dReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'segmentation_models_pytorch.base.modules.Activation' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load(f'pretrained_models/{experiment_name}_weights.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL6zv9Byjm-5",
        "outputId": "acc0d926-8f07-4450-ef8e-5be7948df79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<All keys matched successfully>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(experiment_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsyvv_P-9zZL",
        "outputId": "04383dfc-aa00-4021-a6bf-aceee977b4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cars/sdd/raw/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(df_test, params, image_path=TEST_IMAGE_PATH,\n",
        "               batch_size=BATCH_SIZE, rounds=ROUNDS, \n",
        "               num_goals=NUM_GOALS, num_traj=NUM_TRAJ, device=None, dataset_name=DATASET_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boaM0aHQjoQj",
        "outputId": "c37402d7-7494-411c-e3bb-a8d57e5710ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Prepare Dataset: 100%|██████████| 6/6 [00:00<00:00, 412.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Device is : cuda\n",
            "Preprocess data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/content/drive/MyDrive/YNet/utils/dataloader.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(trajectories), meta, scene_list\n",
            "\rRound:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start testing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Round: 100%|██████████| 1/1 [09:03<00:00, 543.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 0: \n",
            "Test ADE: 83.24583435058594 \n",
            "Test FDE: 62.058013916015625\n",
            "\n",
            "\n",
            "Average performance over 1 rounds: \n",
            "Test ADE: 83.24583435058594 \n",
            "Test FDE: 62.058013916015625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}